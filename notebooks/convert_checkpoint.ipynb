{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9b34deb-0f53-43d3-a607-f8564afea374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/jmain02/home/J2AD007/txk47/cxz00-txk47/cliport\")\n",
    "sys.path.append(\"/jmain02/home/J2AD007/txk47/cxz00-txk47/cliport/mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "053a003d-9431-4b24-af54-87c00c359d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models_lib\n",
    "import util.misc as misc\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7edf43e8-defb-44eb-afd0-0476941bfab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models_lib.__dict__['mae_robot_lang'](norm_pix_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b761e03b-c9c3-478e-a268-979ea3be5396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1141508/4050986575.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_dict = torch.load(checkpoint_path)\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"/jmain02/home/J2AD007/txk47/cxz00-txk47/cliport/checkpoints/clip_vit_base_patch16.pth\"\n",
    "checkpoint_dict = torch.load(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "927e78a4-1cb4-4ae0-9d41-1c3b41f5912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_checkpoint(state_dict):\n",
    "    for key, value in state_dict.items():\n",
    "         print(f'{key} : {value.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95bd7622-774a-40d7-909b-2ff04372d2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token : torch.Size([1, 1, 768])\n",
      "pos_embed : torch.Size([1, 201, 768])\n",
      "mask_token : torch.Size([1, 1, 512])\n",
      "decoder_pos_embed : torch.Size([1, 201, 512])\n",
      "patch_embed.proj.weight : torch.Size([768, 3, 16, 16])\n",
      "patch_embed.proj.bias : torch.Size([768])\n",
      "blocks.0.norm1.weight : torch.Size([768])\n",
      "blocks.0.norm1.bias : torch.Size([768])\n",
      "blocks.0.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.0.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.0.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.0.attn.proj.bias : torch.Size([768])\n",
      "blocks.0.norm2.weight : torch.Size([768])\n",
      "blocks.0.norm2.bias : torch.Size([768])\n",
      "blocks.0.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.0.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.0.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.0.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.1.norm1.weight : torch.Size([768])\n",
      "blocks.1.norm1.bias : torch.Size([768])\n",
      "blocks.1.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.1.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.1.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.1.attn.proj.bias : torch.Size([768])\n",
      "blocks.1.norm2.weight : torch.Size([768])\n",
      "blocks.1.norm2.bias : torch.Size([768])\n",
      "blocks.1.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.1.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.1.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.1.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.2.norm1.weight : torch.Size([768])\n",
      "blocks.2.norm1.bias : torch.Size([768])\n",
      "blocks.2.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.2.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.2.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.2.attn.proj.bias : torch.Size([768])\n",
      "blocks.2.norm2.weight : torch.Size([768])\n",
      "blocks.2.norm2.bias : torch.Size([768])\n",
      "blocks.2.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.2.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.2.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.2.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.3.norm1.weight : torch.Size([768])\n",
      "blocks.3.norm1.bias : torch.Size([768])\n",
      "blocks.3.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.3.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.3.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.3.attn.proj.bias : torch.Size([768])\n",
      "blocks.3.norm2.weight : torch.Size([768])\n",
      "blocks.3.norm2.bias : torch.Size([768])\n",
      "blocks.3.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.3.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.3.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.3.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.4.norm1.weight : torch.Size([768])\n",
      "blocks.4.norm1.bias : torch.Size([768])\n",
      "blocks.4.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.4.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.4.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.4.attn.proj.bias : torch.Size([768])\n",
      "blocks.4.norm2.weight : torch.Size([768])\n",
      "blocks.4.norm2.bias : torch.Size([768])\n",
      "blocks.4.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.4.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.4.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.4.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.5.norm1.weight : torch.Size([768])\n",
      "blocks.5.norm1.bias : torch.Size([768])\n",
      "blocks.5.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.5.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.5.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.5.attn.proj.bias : torch.Size([768])\n",
      "blocks.5.norm2.weight : torch.Size([768])\n",
      "blocks.5.norm2.bias : torch.Size([768])\n",
      "blocks.5.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.5.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.5.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.5.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.6.norm1.weight : torch.Size([768])\n",
      "blocks.6.norm1.bias : torch.Size([768])\n",
      "blocks.6.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.6.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.6.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.6.attn.proj.bias : torch.Size([768])\n",
      "blocks.6.norm2.weight : torch.Size([768])\n",
      "blocks.6.norm2.bias : torch.Size([768])\n",
      "blocks.6.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.6.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.6.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.6.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.7.norm1.weight : torch.Size([768])\n",
      "blocks.7.norm1.bias : torch.Size([768])\n",
      "blocks.7.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.7.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.7.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.7.attn.proj.bias : torch.Size([768])\n",
      "blocks.7.norm2.weight : torch.Size([768])\n",
      "blocks.7.norm2.bias : torch.Size([768])\n",
      "blocks.7.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.7.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.7.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.7.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.8.norm1.weight : torch.Size([768])\n",
      "blocks.8.norm1.bias : torch.Size([768])\n",
      "blocks.8.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.8.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.8.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.8.attn.proj.bias : torch.Size([768])\n",
      "blocks.8.norm2.weight : torch.Size([768])\n",
      "blocks.8.norm2.bias : torch.Size([768])\n",
      "blocks.8.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.8.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.8.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.8.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.9.norm1.weight : torch.Size([768])\n",
      "blocks.9.norm1.bias : torch.Size([768])\n",
      "blocks.9.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.9.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.9.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.9.attn.proj.bias : torch.Size([768])\n",
      "blocks.9.norm2.weight : torch.Size([768])\n",
      "blocks.9.norm2.bias : torch.Size([768])\n",
      "blocks.9.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.9.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.9.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.9.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.10.norm1.weight : torch.Size([768])\n",
      "blocks.10.norm1.bias : torch.Size([768])\n",
      "blocks.10.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.10.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.10.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.10.attn.proj.bias : torch.Size([768])\n",
      "blocks.10.norm2.weight : torch.Size([768])\n",
      "blocks.10.norm2.bias : torch.Size([768])\n",
      "blocks.10.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.10.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.10.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.10.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.11.norm1.weight : torch.Size([768])\n",
      "blocks.11.norm1.bias : torch.Size([768])\n",
      "blocks.11.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.11.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.11.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.11.attn.proj.bias : torch.Size([768])\n",
      "blocks.11.norm2.weight : torch.Size([768])\n",
      "blocks.11.norm2.bias : torch.Size([768])\n",
      "blocks.11.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.11.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.11.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.11.mlp.fc2.bias : torch.Size([768])\n",
      "norm.weight : torch.Size([768])\n",
      "norm.bias : torch.Size([768])\n",
      "decoder_embed.weight : torch.Size([512, 768])\n",
      "decoder_embed.bias : torch.Size([512])\n",
      "decoder_blocks.0.norm1.weight : torch.Size([512])\n",
      "decoder_blocks.0.norm1.bias : torch.Size([512])\n",
      "decoder_blocks.0.attn.qkv.weight : torch.Size([1536, 512])\n",
      "decoder_blocks.0.attn.qkv.bias : torch.Size([1536])\n",
      "decoder_blocks.0.attn.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.0.attn.proj.bias : torch.Size([512])\n",
      "decoder_blocks.0.cross_attn_img.projq.weight : torch.Size([512, 512])\n",
      "decoder_blocks.0.cross_attn_img.projq.bias : torch.Size([512])\n",
      "decoder_blocks.0.cross_attn_img.projk.weight : torch.Size([512, 512])\n",
      "decoder_blocks.0.cross_attn_img.projk.bias : torch.Size([512])\n",
      "decoder_blocks.0.cross_attn_img.projv.weight : torch.Size([512, 512])\n",
      "decoder_blocks.0.cross_attn_img.projv.bias : torch.Size([512])\n",
      "decoder_blocks.0.cross_attn_img.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.0.cross_attn_img.proj.bias : torch.Size([512])\n",
      "decoder_blocks.0.cross_attn_lang.projq.weight : torch.Size([512, 512])\n",
      "decoder_blocks.0.cross_attn_lang.projq.bias : torch.Size([512])\n",
      "decoder_blocks.0.cross_attn_lang.projk.weight : torch.Size([512, 512])\n",
      "decoder_blocks.0.cross_attn_lang.projk.bias : torch.Size([512])\n",
      "decoder_blocks.0.cross_attn_lang.projv.weight : torch.Size([512, 512])\n",
      "decoder_blocks.0.cross_attn_lang.projv.bias : torch.Size([512])\n",
      "decoder_blocks.0.cross_attn_lang.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.0.cross_attn_lang.proj.bias : torch.Size([512])\n",
      "decoder_blocks.0.norm2.weight : torch.Size([512])\n",
      "decoder_blocks.0.norm2.bias : torch.Size([512])\n",
      "decoder_blocks.0.norm3.weight : torch.Size([512])\n",
      "decoder_blocks.0.norm3.bias : torch.Size([512])\n",
      "decoder_blocks.0.norm4.weight : torch.Size([512])\n",
      "decoder_blocks.0.norm4.bias : torch.Size([512])\n",
      "decoder_blocks.0.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "decoder_blocks.0.mlp.fc1.bias : torch.Size([2048])\n",
      "decoder_blocks.0.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "decoder_blocks.0.mlp.fc2.bias : torch.Size([512])\n",
      "decoder_blocks.0.norm_y.weight : torch.Size([512])\n",
      "decoder_blocks.0.norm_y.bias : torch.Size([512])\n",
      "decoder_blocks.1.norm1.weight : torch.Size([512])\n",
      "decoder_blocks.1.norm1.bias : torch.Size([512])\n",
      "decoder_blocks.1.attn.qkv.weight : torch.Size([1536, 512])\n",
      "decoder_blocks.1.attn.qkv.bias : torch.Size([1536])\n",
      "decoder_blocks.1.attn.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.1.attn.proj.bias : torch.Size([512])\n",
      "decoder_blocks.1.cross_attn_img.projq.weight : torch.Size([512, 512])\n",
      "decoder_blocks.1.cross_attn_img.projq.bias : torch.Size([512])\n",
      "decoder_blocks.1.cross_attn_img.projk.weight : torch.Size([512, 512])\n",
      "decoder_blocks.1.cross_attn_img.projk.bias : torch.Size([512])\n",
      "decoder_blocks.1.cross_attn_img.projv.weight : torch.Size([512, 512])\n",
      "decoder_blocks.1.cross_attn_img.projv.bias : torch.Size([512])\n",
      "decoder_blocks.1.cross_attn_img.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.1.cross_attn_img.proj.bias : torch.Size([512])\n",
      "decoder_blocks.1.cross_attn_lang.projq.weight : torch.Size([512, 512])\n",
      "decoder_blocks.1.cross_attn_lang.projq.bias : torch.Size([512])\n",
      "decoder_blocks.1.cross_attn_lang.projk.weight : torch.Size([512, 512])\n",
      "decoder_blocks.1.cross_attn_lang.projk.bias : torch.Size([512])\n",
      "decoder_blocks.1.cross_attn_lang.projv.weight : torch.Size([512, 512])\n",
      "decoder_blocks.1.cross_attn_lang.projv.bias : torch.Size([512])\n",
      "decoder_blocks.1.cross_attn_lang.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.1.cross_attn_lang.proj.bias : torch.Size([512])\n",
      "decoder_blocks.1.norm2.weight : torch.Size([512])\n",
      "decoder_blocks.1.norm2.bias : torch.Size([512])\n",
      "decoder_blocks.1.norm3.weight : torch.Size([512])\n",
      "decoder_blocks.1.norm3.bias : torch.Size([512])\n",
      "decoder_blocks.1.norm4.weight : torch.Size([512])\n",
      "decoder_blocks.1.norm4.bias : torch.Size([512])\n",
      "decoder_blocks.1.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "decoder_blocks.1.mlp.fc1.bias : torch.Size([2048])\n",
      "decoder_blocks.1.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "decoder_blocks.1.mlp.fc2.bias : torch.Size([512])\n",
      "decoder_blocks.1.norm_y.weight : torch.Size([512])\n",
      "decoder_blocks.1.norm_y.bias : torch.Size([512])\n",
      "decoder_blocks.2.norm1.weight : torch.Size([512])\n",
      "decoder_blocks.2.norm1.bias : torch.Size([512])\n",
      "decoder_blocks.2.attn.qkv.weight : torch.Size([1536, 512])\n",
      "decoder_blocks.2.attn.qkv.bias : torch.Size([1536])\n",
      "decoder_blocks.2.attn.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.2.attn.proj.bias : torch.Size([512])\n",
      "decoder_blocks.2.cross_attn_img.projq.weight : torch.Size([512, 512])\n",
      "decoder_blocks.2.cross_attn_img.projq.bias : torch.Size([512])\n",
      "decoder_blocks.2.cross_attn_img.projk.weight : torch.Size([512, 512])\n",
      "decoder_blocks.2.cross_attn_img.projk.bias : torch.Size([512])\n",
      "decoder_blocks.2.cross_attn_img.projv.weight : torch.Size([512, 512])\n",
      "decoder_blocks.2.cross_attn_img.projv.bias : torch.Size([512])\n",
      "decoder_blocks.2.cross_attn_img.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.2.cross_attn_img.proj.bias : torch.Size([512])\n",
      "decoder_blocks.2.cross_attn_lang.projq.weight : torch.Size([512, 512])\n",
      "decoder_blocks.2.cross_attn_lang.projq.bias : torch.Size([512])\n",
      "decoder_blocks.2.cross_attn_lang.projk.weight : torch.Size([512, 512])\n",
      "decoder_blocks.2.cross_attn_lang.projk.bias : torch.Size([512])\n",
      "decoder_blocks.2.cross_attn_lang.projv.weight : torch.Size([512, 512])\n",
      "decoder_blocks.2.cross_attn_lang.projv.bias : torch.Size([512])\n",
      "decoder_blocks.2.cross_attn_lang.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.2.cross_attn_lang.proj.bias : torch.Size([512])\n",
      "decoder_blocks.2.norm2.weight : torch.Size([512])\n",
      "decoder_blocks.2.norm2.bias : torch.Size([512])\n",
      "decoder_blocks.2.norm3.weight : torch.Size([512])\n",
      "decoder_blocks.2.norm3.bias : torch.Size([512])\n",
      "decoder_blocks.2.norm4.weight : torch.Size([512])\n",
      "decoder_blocks.2.norm4.bias : torch.Size([512])\n",
      "decoder_blocks.2.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "decoder_blocks.2.mlp.fc1.bias : torch.Size([2048])\n",
      "decoder_blocks.2.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "decoder_blocks.2.mlp.fc2.bias : torch.Size([512])\n",
      "decoder_blocks.2.norm_y.weight : torch.Size([512])\n",
      "decoder_blocks.2.norm_y.bias : torch.Size([512])\n",
      "decoder_blocks.3.norm1.weight : torch.Size([512])\n",
      "decoder_blocks.3.norm1.bias : torch.Size([512])\n",
      "decoder_blocks.3.attn.qkv.weight : torch.Size([1536, 512])\n",
      "decoder_blocks.3.attn.qkv.bias : torch.Size([1536])\n",
      "decoder_blocks.3.attn.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.3.attn.proj.bias : torch.Size([512])\n",
      "decoder_blocks.3.cross_attn_img.projq.weight : torch.Size([512, 512])\n",
      "decoder_blocks.3.cross_attn_img.projq.bias : torch.Size([512])\n",
      "decoder_blocks.3.cross_attn_img.projk.weight : torch.Size([512, 512])\n",
      "decoder_blocks.3.cross_attn_img.projk.bias : torch.Size([512])\n",
      "decoder_blocks.3.cross_attn_img.projv.weight : torch.Size([512, 512])\n",
      "decoder_blocks.3.cross_attn_img.projv.bias : torch.Size([512])\n",
      "decoder_blocks.3.cross_attn_img.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.3.cross_attn_img.proj.bias : torch.Size([512])\n",
      "decoder_blocks.3.cross_attn_lang.projq.weight : torch.Size([512, 512])\n",
      "decoder_blocks.3.cross_attn_lang.projq.bias : torch.Size([512])\n",
      "decoder_blocks.3.cross_attn_lang.projk.weight : torch.Size([512, 512])\n",
      "decoder_blocks.3.cross_attn_lang.projk.bias : torch.Size([512])\n",
      "decoder_blocks.3.cross_attn_lang.projv.weight : torch.Size([512, 512])\n",
      "decoder_blocks.3.cross_attn_lang.projv.bias : torch.Size([512])\n",
      "decoder_blocks.3.cross_attn_lang.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.3.cross_attn_lang.proj.bias : torch.Size([512])\n",
      "decoder_blocks.3.norm2.weight : torch.Size([512])\n",
      "decoder_blocks.3.norm2.bias : torch.Size([512])\n",
      "decoder_blocks.3.norm3.weight : torch.Size([512])\n",
      "decoder_blocks.3.norm3.bias : torch.Size([512])\n",
      "decoder_blocks.3.norm4.weight : torch.Size([512])\n",
      "decoder_blocks.3.norm4.bias : torch.Size([512])\n",
      "decoder_blocks.3.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "decoder_blocks.3.mlp.fc1.bias : torch.Size([2048])\n",
      "decoder_blocks.3.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "decoder_blocks.3.mlp.fc2.bias : torch.Size([512])\n",
      "decoder_blocks.3.norm_y.weight : torch.Size([512])\n",
      "decoder_blocks.3.norm_y.bias : torch.Size([512])\n",
      "decoder_blocks.4.norm1.weight : torch.Size([512])\n",
      "decoder_blocks.4.norm1.bias : torch.Size([512])\n",
      "decoder_blocks.4.attn.qkv.weight : torch.Size([1536, 512])\n",
      "decoder_blocks.4.attn.qkv.bias : torch.Size([1536])\n",
      "decoder_blocks.4.attn.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.4.attn.proj.bias : torch.Size([512])\n",
      "decoder_blocks.4.cross_attn_img.projq.weight : torch.Size([512, 512])\n",
      "decoder_blocks.4.cross_attn_img.projq.bias : torch.Size([512])\n",
      "decoder_blocks.4.cross_attn_img.projk.weight : torch.Size([512, 512])\n",
      "decoder_blocks.4.cross_attn_img.projk.bias : torch.Size([512])\n",
      "decoder_blocks.4.cross_attn_img.projv.weight : torch.Size([512, 512])\n",
      "decoder_blocks.4.cross_attn_img.projv.bias : torch.Size([512])\n",
      "decoder_blocks.4.cross_attn_img.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.4.cross_attn_img.proj.bias : torch.Size([512])\n",
      "decoder_blocks.4.cross_attn_lang.projq.weight : torch.Size([512, 512])\n",
      "decoder_blocks.4.cross_attn_lang.projq.bias : torch.Size([512])\n",
      "decoder_blocks.4.cross_attn_lang.projk.weight : torch.Size([512, 512])\n",
      "decoder_blocks.4.cross_attn_lang.projk.bias : torch.Size([512])\n",
      "decoder_blocks.4.cross_attn_lang.projv.weight : torch.Size([512, 512])\n",
      "decoder_blocks.4.cross_attn_lang.projv.bias : torch.Size([512])\n",
      "decoder_blocks.4.cross_attn_lang.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.4.cross_attn_lang.proj.bias : torch.Size([512])\n",
      "decoder_blocks.4.norm2.weight : torch.Size([512])\n",
      "decoder_blocks.4.norm2.bias : torch.Size([512])\n",
      "decoder_blocks.4.norm3.weight : torch.Size([512])\n",
      "decoder_blocks.4.norm3.bias : torch.Size([512])\n",
      "decoder_blocks.4.norm4.weight : torch.Size([512])\n",
      "decoder_blocks.4.norm4.bias : torch.Size([512])\n",
      "decoder_blocks.4.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "decoder_blocks.4.mlp.fc1.bias : torch.Size([2048])\n",
      "decoder_blocks.4.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "decoder_blocks.4.mlp.fc2.bias : torch.Size([512])\n",
      "decoder_blocks.4.norm_y.weight : torch.Size([512])\n",
      "decoder_blocks.4.norm_y.bias : torch.Size([512])\n",
      "decoder_blocks.5.norm1.weight : torch.Size([512])\n",
      "decoder_blocks.5.norm1.bias : torch.Size([512])\n",
      "decoder_blocks.5.attn.qkv.weight : torch.Size([1536, 512])\n",
      "decoder_blocks.5.attn.qkv.bias : torch.Size([1536])\n",
      "decoder_blocks.5.attn.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.5.attn.proj.bias : torch.Size([512])\n",
      "decoder_blocks.5.cross_attn_img.projq.weight : torch.Size([512, 512])\n",
      "decoder_blocks.5.cross_attn_img.projq.bias : torch.Size([512])\n",
      "decoder_blocks.5.cross_attn_img.projk.weight : torch.Size([512, 512])\n",
      "decoder_blocks.5.cross_attn_img.projk.bias : torch.Size([512])\n",
      "decoder_blocks.5.cross_attn_img.projv.weight : torch.Size([512, 512])\n",
      "decoder_blocks.5.cross_attn_img.projv.bias : torch.Size([512])\n",
      "decoder_blocks.5.cross_attn_img.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.5.cross_attn_img.proj.bias : torch.Size([512])\n",
      "decoder_blocks.5.cross_attn_lang.projq.weight : torch.Size([512, 512])\n",
      "decoder_blocks.5.cross_attn_lang.projq.bias : torch.Size([512])\n",
      "decoder_blocks.5.cross_attn_lang.projk.weight : torch.Size([512, 512])\n",
      "decoder_blocks.5.cross_attn_lang.projk.bias : torch.Size([512])\n",
      "decoder_blocks.5.cross_attn_lang.projv.weight : torch.Size([512, 512])\n",
      "decoder_blocks.5.cross_attn_lang.projv.bias : torch.Size([512])\n",
      "decoder_blocks.5.cross_attn_lang.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.5.cross_attn_lang.proj.bias : torch.Size([512])\n",
      "decoder_blocks.5.norm2.weight : torch.Size([512])\n",
      "decoder_blocks.5.norm2.bias : torch.Size([512])\n",
      "decoder_blocks.5.norm3.weight : torch.Size([512])\n",
      "decoder_blocks.5.norm3.bias : torch.Size([512])\n",
      "decoder_blocks.5.norm4.weight : torch.Size([512])\n",
      "decoder_blocks.5.norm4.bias : torch.Size([512])\n",
      "decoder_blocks.5.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "decoder_blocks.5.mlp.fc1.bias : torch.Size([2048])\n",
      "decoder_blocks.5.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "decoder_blocks.5.mlp.fc2.bias : torch.Size([512])\n",
      "decoder_blocks.5.norm_y.weight : torch.Size([512])\n",
      "decoder_blocks.5.norm_y.bias : torch.Size([512])\n",
      "decoder_blocks.6.norm1.weight : torch.Size([512])\n",
      "decoder_blocks.6.norm1.bias : torch.Size([512])\n",
      "decoder_blocks.6.attn.qkv.weight : torch.Size([1536, 512])\n",
      "decoder_blocks.6.attn.qkv.bias : torch.Size([1536])\n",
      "decoder_blocks.6.attn.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.6.attn.proj.bias : torch.Size([512])\n",
      "decoder_blocks.6.cross_attn_img.projq.weight : torch.Size([512, 512])\n",
      "decoder_blocks.6.cross_attn_img.projq.bias : torch.Size([512])\n",
      "decoder_blocks.6.cross_attn_img.projk.weight : torch.Size([512, 512])\n",
      "decoder_blocks.6.cross_attn_img.projk.bias : torch.Size([512])\n",
      "decoder_blocks.6.cross_attn_img.projv.weight : torch.Size([512, 512])\n",
      "decoder_blocks.6.cross_attn_img.projv.bias : torch.Size([512])\n",
      "decoder_blocks.6.cross_attn_img.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.6.cross_attn_img.proj.bias : torch.Size([512])\n",
      "decoder_blocks.6.cross_attn_lang.projq.weight : torch.Size([512, 512])\n",
      "decoder_blocks.6.cross_attn_lang.projq.bias : torch.Size([512])\n",
      "decoder_blocks.6.cross_attn_lang.projk.weight : torch.Size([512, 512])\n",
      "decoder_blocks.6.cross_attn_lang.projk.bias : torch.Size([512])\n",
      "decoder_blocks.6.cross_attn_lang.projv.weight : torch.Size([512, 512])\n",
      "decoder_blocks.6.cross_attn_lang.projv.bias : torch.Size([512])\n",
      "decoder_blocks.6.cross_attn_lang.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.6.cross_attn_lang.proj.bias : torch.Size([512])\n",
      "decoder_blocks.6.norm2.weight : torch.Size([512])\n",
      "decoder_blocks.6.norm2.bias : torch.Size([512])\n",
      "decoder_blocks.6.norm3.weight : torch.Size([512])\n",
      "decoder_blocks.6.norm3.bias : torch.Size([512])\n",
      "decoder_blocks.6.norm4.weight : torch.Size([512])\n",
      "decoder_blocks.6.norm4.bias : torch.Size([512])\n",
      "decoder_blocks.6.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "decoder_blocks.6.mlp.fc1.bias : torch.Size([2048])\n",
      "decoder_blocks.6.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "decoder_blocks.6.mlp.fc2.bias : torch.Size([512])\n",
      "decoder_blocks.6.norm_y.weight : torch.Size([512])\n",
      "decoder_blocks.6.norm_y.bias : torch.Size([512])\n",
      "decoder_blocks.7.norm1.weight : torch.Size([512])\n",
      "decoder_blocks.7.norm1.bias : torch.Size([512])\n",
      "decoder_blocks.7.attn.qkv.weight : torch.Size([1536, 512])\n",
      "decoder_blocks.7.attn.qkv.bias : torch.Size([1536])\n",
      "decoder_blocks.7.attn.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.7.attn.proj.bias : torch.Size([512])\n",
      "decoder_blocks.7.cross_attn_img.projq.weight : torch.Size([512, 512])\n",
      "decoder_blocks.7.cross_attn_img.projq.bias : torch.Size([512])\n",
      "decoder_blocks.7.cross_attn_img.projk.weight : torch.Size([512, 512])\n",
      "decoder_blocks.7.cross_attn_img.projk.bias : torch.Size([512])\n",
      "decoder_blocks.7.cross_attn_img.projv.weight : torch.Size([512, 512])\n",
      "decoder_blocks.7.cross_attn_img.projv.bias : torch.Size([512])\n",
      "decoder_blocks.7.cross_attn_img.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.7.cross_attn_img.proj.bias : torch.Size([512])\n",
      "decoder_blocks.7.cross_attn_lang.projq.weight : torch.Size([512, 512])\n",
      "decoder_blocks.7.cross_attn_lang.projq.bias : torch.Size([512])\n",
      "decoder_blocks.7.cross_attn_lang.projk.weight : torch.Size([512, 512])\n",
      "decoder_blocks.7.cross_attn_lang.projk.bias : torch.Size([512])\n",
      "decoder_blocks.7.cross_attn_lang.projv.weight : torch.Size([512, 512])\n",
      "decoder_blocks.7.cross_attn_lang.projv.bias : torch.Size([512])\n",
      "decoder_blocks.7.cross_attn_lang.proj.weight : torch.Size([512, 512])\n",
      "decoder_blocks.7.cross_attn_lang.proj.bias : torch.Size([512])\n",
      "decoder_blocks.7.norm2.weight : torch.Size([512])\n",
      "decoder_blocks.7.norm2.bias : torch.Size([512])\n",
      "decoder_blocks.7.norm3.weight : torch.Size([512])\n",
      "decoder_blocks.7.norm3.bias : torch.Size([512])\n",
      "decoder_blocks.7.norm4.weight : torch.Size([512])\n",
      "decoder_blocks.7.norm4.bias : torch.Size([512])\n",
      "decoder_blocks.7.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "decoder_blocks.7.mlp.fc1.bias : torch.Size([2048])\n",
      "decoder_blocks.7.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "decoder_blocks.7.mlp.fc2.bias : torch.Size([512])\n",
      "decoder_blocks.7.norm_y.weight : torch.Size([512])\n",
      "decoder_blocks.7.norm_y.bias : torch.Size([512])\n",
      "decoder_norm.weight : torch.Size([512])\n",
      "decoder_norm.bias : torch.Size([512])\n",
      "decoder_pred.weight : torch.Size([768, 512])\n",
      "decoder_pred.bias : torch.Size([768])\n",
      "clip_text.text_model.embeddings.token_embedding.weight : torch.Size([49408, 512])\n",
      "clip_text.text_model.embeddings.position_embedding.weight : torch.Size([77, 512])\n",
      "clip_text.text_model.encoder.layers.0.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.0.self_attn.k_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.0.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.0.self_attn.v_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.0.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.0.self_attn.q_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.0.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.0.self_attn.out_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.0.layer_norm1.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.0.layer_norm1.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.0.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "clip_text.text_model.encoder.layers.0.mlp.fc1.bias : torch.Size([2048])\n",
      "clip_text.text_model.encoder.layers.0.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "clip_text.text_model.encoder.layers.0.mlp.fc2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.0.layer_norm2.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.0.layer_norm2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.1.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.1.self_attn.k_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.1.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.1.self_attn.v_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.1.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.1.self_attn.q_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.1.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.1.self_attn.out_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.1.layer_norm1.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.1.layer_norm1.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.1.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "clip_text.text_model.encoder.layers.1.mlp.fc1.bias : torch.Size([2048])\n",
      "clip_text.text_model.encoder.layers.1.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "clip_text.text_model.encoder.layers.1.mlp.fc2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.1.layer_norm2.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.1.layer_norm2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.2.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.2.self_attn.k_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.2.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.2.self_attn.v_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.2.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.2.self_attn.q_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.2.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.2.self_attn.out_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.2.layer_norm1.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.2.layer_norm1.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.2.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "clip_text.text_model.encoder.layers.2.mlp.fc1.bias : torch.Size([2048])\n",
      "clip_text.text_model.encoder.layers.2.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "clip_text.text_model.encoder.layers.2.mlp.fc2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.2.layer_norm2.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.2.layer_norm2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.3.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.3.self_attn.k_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.3.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.3.self_attn.v_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.3.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.3.self_attn.q_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.3.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.3.self_attn.out_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.3.layer_norm1.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.3.layer_norm1.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.3.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "clip_text.text_model.encoder.layers.3.mlp.fc1.bias : torch.Size([2048])\n",
      "clip_text.text_model.encoder.layers.3.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "clip_text.text_model.encoder.layers.3.mlp.fc2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.3.layer_norm2.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.3.layer_norm2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.4.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.4.self_attn.k_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.4.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.4.self_attn.v_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.4.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.4.self_attn.q_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.4.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.4.self_attn.out_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.4.layer_norm1.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.4.layer_norm1.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.4.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "clip_text.text_model.encoder.layers.4.mlp.fc1.bias : torch.Size([2048])\n",
      "clip_text.text_model.encoder.layers.4.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "clip_text.text_model.encoder.layers.4.mlp.fc2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.4.layer_norm2.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.4.layer_norm2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.5.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.5.self_attn.k_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.5.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.5.self_attn.v_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.5.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.5.self_attn.q_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.5.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.5.self_attn.out_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.5.layer_norm1.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.5.layer_norm1.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.5.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "clip_text.text_model.encoder.layers.5.mlp.fc1.bias : torch.Size([2048])\n",
      "clip_text.text_model.encoder.layers.5.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "clip_text.text_model.encoder.layers.5.mlp.fc2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.5.layer_norm2.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.5.layer_norm2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.6.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.6.self_attn.k_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.6.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.6.self_attn.v_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.6.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.6.self_attn.q_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.6.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.6.self_attn.out_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.6.layer_norm1.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.6.layer_norm1.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.6.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "clip_text.text_model.encoder.layers.6.mlp.fc1.bias : torch.Size([2048])\n",
      "clip_text.text_model.encoder.layers.6.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "clip_text.text_model.encoder.layers.6.mlp.fc2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.6.layer_norm2.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.6.layer_norm2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.7.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.7.self_attn.k_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.7.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.7.self_attn.v_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.7.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.7.self_attn.q_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.7.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.7.self_attn.out_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.7.layer_norm1.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.7.layer_norm1.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.7.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "clip_text.text_model.encoder.layers.7.mlp.fc1.bias : torch.Size([2048])\n",
      "clip_text.text_model.encoder.layers.7.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "clip_text.text_model.encoder.layers.7.mlp.fc2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.7.layer_norm2.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.7.layer_norm2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.8.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.8.self_attn.k_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.8.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.8.self_attn.v_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.8.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.8.self_attn.q_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.8.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.8.self_attn.out_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.8.layer_norm1.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.8.layer_norm1.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.8.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "clip_text.text_model.encoder.layers.8.mlp.fc1.bias : torch.Size([2048])\n",
      "clip_text.text_model.encoder.layers.8.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "clip_text.text_model.encoder.layers.8.mlp.fc2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.8.layer_norm2.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.8.layer_norm2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.9.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.9.self_attn.k_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.9.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.9.self_attn.v_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.9.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.9.self_attn.q_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.9.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.9.self_attn.out_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.9.layer_norm1.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.9.layer_norm1.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.9.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "clip_text.text_model.encoder.layers.9.mlp.fc1.bias : torch.Size([2048])\n",
      "clip_text.text_model.encoder.layers.9.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "clip_text.text_model.encoder.layers.9.mlp.fc2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.9.layer_norm2.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.9.layer_norm2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.10.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.10.self_attn.k_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.10.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.10.self_attn.v_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.10.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.10.self_attn.q_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.10.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.10.self_attn.out_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.10.layer_norm1.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.10.layer_norm1.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.10.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "clip_text.text_model.encoder.layers.10.mlp.fc1.bias : torch.Size([2048])\n",
      "clip_text.text_model.encoder.layers.10.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "clip_text.text_model.encoder.layers.10.mlp.fc2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.10.layer_norm2.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.10.layer_norm2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.11.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.11.self_attn.k_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.11.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.11.self_attn.v_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.11.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.11.self_attn.q_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.11.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "clip_text.text_model.encoder.layers.11.self_attn.out_proj.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.11.layer_norm1.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.11.layer_norm1.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.11.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "clip_text.text_model.encoder.layers.11.mlp.fc1.bias : torch.Size([2048])\n",
      "clip_text.text_model.encoder.layers.11.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "clip_text.text_model.encoder.layers.11.mlp.fc2.bias : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.11.layer_norm2.weight : torch.Size([512])\n",
      "clip_text.text_model.encoder.layers.11.layer_norm2.bias : torch.Size([512])\n",
      "clip_text.text_model.final_layer_norm.weight : torch.Size([512])\n",
      "clip_text.text_model.final_layer_norm.bias : torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "print_checkpoint(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1fb810b9-8dc3-4177-a7af-37a4716e1631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_scale : torch.Size([])\n",
      "text_model.embeddings.token_embedding.weight : torch.Size([49408, 512])\n",
      "text_model.embeddings.position_embedding.weight : torch.Size([77, 512])\n",
      "text_model.encoder.layers.0.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.0.self_attn.k_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.0.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.0.self_attn.v_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.0.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.0.self_attn.q_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.0.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.0.self_attn.out_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.0.layer_norm1.weight : torch.Size([512])\n",
      "text_model.encoder.layers.0.layer_norm1.bias : torch.Size([512])\n",
      "text_model.encoder.layers.0.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "text_model.encoder.layers.0.mlp.fc1.bias : torch.Size([2048])\n",
      "text_model.encoder.layers.0.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "text_model.encoder.layers.0.mlp.fc2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.0.layer_norm2.weight : torch.Size([512])\n",
      "text_model.encoder.layers.0.layer_norm2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.1.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.1.self_attn.k_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.1.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.1.self_attn.v_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.1.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.1.self_attn.q_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.1.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.1.self_attn.out_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.1.layer_norm1.weight : torch.Size([512])\n",
      "text_model.encoder.layers.1.layer_norm1.bias : torch.Size([512])\n",
      "text_model.encoder.layers.1.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "text_model.encoder.layers.1.mlp.fc1.bias : torch.Size([2048])\n",
      "text_model.encoder.layers.1.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "text_model.encoder.layers.1.mlp.fc2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.1.layer_norm2.weight : torch.Size([512])\n",
      "text_model.encoder.layers.1.layer_norm2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.2.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.2.self_attn.k_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.2.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.2.self_attn.v_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.2.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.2.self_attn.q_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.2.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.2.self_attn.out_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.2.layer_norm1.weight : torch.Size([512])\n",
      "text_model.encoder.layers.2.layer_norm1.bias : torch.Size([512])\n",
      "text_model.encoder.layers.2.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "text_model.encoder.layers.2.mlp.fc1.bias : torch.Size([2048])\n",
      "text_model.encoder.layers.2.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "text_model.encoder.layers.2.mlp.fc2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.2.layer_norm2.weight : torch.Size([512])\n",
      "text_model.encoder.layers.2.layer_norm2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.3.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.3.self_attn.k_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.3.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.3.self_attn.v_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.3.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.3.self_attn.q_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.3.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.3.self_attn.out_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.3.layer_norm1.weight : torch.Size([512])\n",
      "text_model.encoder.layers.3.layer_norm1.bias : torch.Size([512])\n",
      "text_model.encoder.layers.3.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "text_model.encoder.layers.3.mlp.fc1.bias : torch.Size([2048])\n",
      "text_model.encoder.layers.3.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "text_model.encoder.layers.3.mlp.fc2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.3.layer_norm2.weight : torch.Size([512])\n",
      "text_model.encoder.layers.3.layer_norm2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.4.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.4.self_attn.k_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.4.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.4.self_attn.v_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.4.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.4.self_attn.q_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.4.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.4.self_attn.out_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.4.layer_norm1.weight : torch.Size([512])\n",
      "text_model.encoder.layers.4.layer_norm1.bias : torch.Size([512])\n",
      "text_model.encoder.layers.4.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "text_model.encoder.layers.4.mlp.fc1.bias : torch.Size([2048])\n",
      "text_model.encoder.layers.4.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "text_model.encoder.layers.4.mlp.fc2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.4.layer_norm2.weight : torch.Size([512])\n",
      "text_model.encoder.layers.4.layer_norm2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.5.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.5.self_attn.k_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.5.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.5.self_attn.v_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.5.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.5.self_attn.q_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.5.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.5.self_attn.out_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.5.layer_norm1.weight : torch.Size([512])\n",
      "text_model.encoder.layers.5.layer_norm1.bias : torch.Size([512])\n",
      "text_model.encoder.layers.5.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "text_model.encoder.layers.5.mlp.fc1.bias : torch.Size([2048])\n",
      "text_model.encoder.layers.5.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "text_model.encoder.layers.5.mlp.fc2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.5.layer_norm2.weight : torch.Size([512])\n",
      "text_model.encoder.layers.5.layer_norm2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.6.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.6.self_attn.k_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.6.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.6.self_attn.v_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.6.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.6.self_attn.q_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.6.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.6.self_attn.out_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.6.layer_norm1.weight : torch.Size([512])\n",
      "text_model.encoder.layers.6.layer_norm1.bias : torch.Size([512])\n",
      "text_model.encoder.layers.6.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "text_model.encoder.layers.6.mlp.fc1.bias : torch.Size([2048])\n",
      "text_model.encoder.layers.6.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "text_model.encoder.layers.6.mlp.fc2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.6.layer_norm2.weight : torch.Size([512])\n",
      "text_model.encoder.layers.6.layer_norm2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.7.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.7.self_attn.k_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.7.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.7.self_attn.v_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.7.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.7.self_attn.q_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.7.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.7.self_attn.out_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.7.layer_norm1.weight : torch.Size([512])\n",
      "text_model.encoder.layers.7.layer_norm1.bias : torch.Size([512])\n",
      "text_model.encoder.layers.7.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "text_model.encoder.layers.7.mlp.fc1.bias : torch.Size([2048])\n",
      "text_model.encoder.layers.7.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "text_model.encoder.layers.7.mlp.fc2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.7.layer_norm2.weight : torch.Size([512])\n",
      "text_model.encoder.layers.7.layer_norm2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.8.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.8.self_attn.k_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.8.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.8.self_attn.v_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.8.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.8.self_attn.q_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.8.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.8.self_attn.out_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.8.layer_norm1.weight : torch.Size([512])\n",
      "text_model.encoder.layers.8.layer_norm1.bias : torch.Size([512])\n",
      "text_model.encoder.layers.8.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "text_model.encoder.layers.8.mlp.fc1.bias : torch.Size([2048])\n",
      "text_model.encoder.layers.8.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "text_model.encoder.layers.8.mlp.fc2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.8.layer_norm2.weight : torch.Size([512])\n",
      "text_model.encoder.layers.8.layer_norm2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.9.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.9.self_attn.k_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.9.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.9.self_attn.v_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.9.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.9.self_attn.q_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.9.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.9.self_attn.out_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.9.layer_norm1.weight : torch.Size([512])\n",
      "text_model.encoder.layers.9.layer_norm1.bias : torch.Size([512])\n",
      "text_model.encoder.layers.9.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "text_model.encoder.layers.9.mlp.fc1.bias : torch.Size([2048])\n",
      "text_model.encoder.layers.9.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "text_model.encoder.layers.9.mlp.fc2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.9.layer_norm2.weight : torch.Size([512])\n",
      "text_model.encoder.layers.9.layer_norm2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.10.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.10.self_attn.k_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.10.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.10.self_attn.v_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.10.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.10.self_attn.q_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.10.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.10.self_attn.out_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.10.layer_norm1.weight : torch.Size([512])\n",
      "text_model.encoder.layers.10.layer_norm1.bias : torch.Size([512])\n",
      "text_model.encoder.layers.10.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "text_model.encoder.layers.10.mlp.fc1.bias : torch.Size([2048])\n",
      "text_model.encoder.layers.10.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "text_model.encoder.layers.10.mlp.fc2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.10.layer_norm2.weight : torch.Size([512])\n",
      "text_model.encoder.layers.10.layer_norm2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.11.self_attn.k_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.11.self_attn.k_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.11.self_attn.v_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.11.self_attn.v_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.11.self_attn.q_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.11.self_attn.q_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.11.self_attn.out_proj.weight : torch.Size([512, 512])\n",
      "text_model.encoder.layers.11.self_attn.out_proj.bias : torch.Size([512])\n",
      "text_model.encoder.layers.11.layer_norm1.weight : torch.Size([512])\n",
      "text_model.encoder.layers.11.layer_norm1.bias : torch.Size([512])\n",
      "text_model.encoder.layers.11.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "text_model.encoder.layers.11.mlp.fc1.bias : torch.Size([2048])\n",
      "text_model.encoder.layers.11.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "text_model.encoder.layers.11.mlp.fc2.bias : torch.Size([512])\n",
      "text_model.encoder.layers.11.layer_norm2.weight : torch.Size([512])\n",
      "text_model.encoder.layers.11.layer_norm2.bias : torch.Size([512])\n",
      "text_model.final_layer_norm.weight : torch.Size([512])\n",
      "text_model.final_layer_norm.bias : torch.Size([512])\n",
      "vision_model.embeddings.class_embedding : torch.Size([768])\n",
      "vision_model.embeddings.patch_embedding.weight : torch.Size([768, 3, 16, 16])\n",
      "vision_model.embeddings.position_embedding.weight : torch.Size([197, 768])\n",
      "vision_model.pre_layrnorm.weight : torch.Size([768])\n",
      "vision_model.pre_layrnorm.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.0.self_attn.k_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.0.self_attn.k_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.0.self_attn.v_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.0.self_attn.v_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.0.self_attn.q_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.0.self_attn.q_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.0.self_attn.out_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.0.self_attn.out_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.0.layer_norm1.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.0.layer_norm1.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.0.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "vision_model.encoder.layers.0.mlp.fc1.bias : torch.Size([3072])\n",
      "vision_model.encoder.layers.0.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "vision_model.encoder.layers.0.mlp.fc2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.0.layer_norm2.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.0.layer_norm2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.1.self_attn.k_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.1.self_attn.k_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.1.self_attn.v_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.1.self_attn.v_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.1.self_attn.q_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.1.self_attn.q_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.1.self_attn.out_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.1.self_attn.out_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.1.layer_norm1.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.1.layer_norm1.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.1.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "vision_model.encoder.layers.1.mlp.fc1.bias : torch.Size([3072])\n",
      "vision_model.encoder.layers.1.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "vision_model.encoder.layers.1.mlp.fc2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.1.layer_norm2.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.1.layer_norm2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.2.self_attn.k_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.2.self_attn.k_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.2.self_attn.v_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.2.self_attn.v_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.2.self_attn.q_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.2.self_attn.q_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.2.self_attn.out_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.2.self_attn.out_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.2.layer_norm1.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.2.layer_norm1.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.2.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "vision_model.encoder.layers.2.mlp.fc1.bias : torch.Size([3072])\n",
      "vision_model.encoder.layers.2.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "vision_model.encoder.layers.2.mlp.fc2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.2.layer_norm2.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.2.layer_norm2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.3.self_attn.k_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.3.self_attn.k_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.3.self_attn.v_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.3.self_attn.v_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.3.self_attn.q_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.3.self_attn.q_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.3.self_attn.out_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.3.self_attn.out_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.3.layer_norm1.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.3.layer_norm1.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.3.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "vision_model.encoder.layers.3.mlp.fc1.bias : torch.Size([3072])\n",
      "vision_model.encoder.layers.3.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "vision_model.encoder.layers.3.mlp.fc2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.3.layer_norm2.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.3.layer_norm2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.4.self_attn.k_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.4.self_attn.k_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.4.self_attn.v_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.4.self_attn.v_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.4.self_attn.q_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.4.self_attn.q_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.4.self_attn.out_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.4.self_attn.out_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.4.layer_norm1.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.4.layer_norm1.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.4.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "vision_model.encoder.layers.4.mlp.fc1.bias : torch.Size([3072])\n",
      "vision_model.encoder.layers.4.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "vision_model.encoder.layers.4.mlp.fc2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.4.layer_norm2.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.4.layer_norm2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.5.self_attn.k_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.5.self_attn.k_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.5.self_attn.v_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.5.self_attn.v_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.5.self_attn.q_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.5.self_attn.q_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.5.self_attn.out_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.5.self_attn.out_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.5.layer_norm1.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.5.layer_norm1.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.5.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "vision_model.encoder.layers.5.mlp.fc1.bias : torch.Size([3072])\n",
      "vision_model.encoder.layers.5.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "vision_model.encoder.layers.5.mlp.fc2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.5.layer_norm2.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.5.layer_norm2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.6.self_attn.k_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.6.self_attn.k_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.6.self_attn.v_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.6.self_attn.v_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.6.self_attn.q_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.6.self_attn.q_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.6.self_attn.out_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.6.self_attn.out_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.6.layer_norm1.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.6.layer_norm1.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.6.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "vision_model.encoder.layers.6.mlp.fc1.bias : torch.Size([3072])\n",
      "vision_model.encoder.layers.6.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "vision_model.encoder.layers.6.mlp.fc2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.6.layer_norm2.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.6.layer_norm2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.7.self_attn.k_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.7.self_attn.k_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.7.self_attn.v_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.7.self_attn.v_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.7.self_attn.q_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.7.self_attn.q_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.7.self_attn.out_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.7.self_attn.out_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.7.layer_norm1.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.7.layer_norm1.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.7.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "vision_model.encoder.layers.7.mlp.fc1.bias : torch.Size([3072])\n",
      "vision_model.encoder.layers.7.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "vision_model.encoder.layers.7.mlp.fc2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.7.layer_norm2.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.7.layer_norm2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.8.self_attn.k_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.8.self_attn.k_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.8.self_attn.v_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.8.self_attn.v_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.8.self_attn.q_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.8.self_attn.q_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.8.self_attn.out_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.8.self_attn.out_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.8.layer_norm1.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.8.layer_norm1.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.8.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "vision_model.encoder.layers.8.mlp.fc1.bias : torch.Size([3072])\n",
      "vision_model.encoder.layers.8.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "vision_model.encoder.layers.8.mlp.fc2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.8.layer_norm2.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.8.layer_norm2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.9.self_attn.k_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.9.self_attn.k_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.9.self_attn.v_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.9.self_attn.v_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.9.self_attn.q_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.9.self_attn.q_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.9.self_attn.out_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.9.self_attn.out_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.9.layer_norm1.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.9.layer_norm1.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.9.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "vision_model.encoder.layers.9.mlp.fc1.bias : torch.Size([3072])\n",
      "vision_model.encoder.layers.9.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "vision_model.encoder.layers.9.mlp.fc2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.9.layer_norm2.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.9.layer_norm2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.10.self_attn.k_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.10.self_attn.k_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.10.self_attn.v_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.10.self_attn.v_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.10.self_attn.q_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.10.self_attn.q_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.10.self_attn.out_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.10.self_attn.out_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.10.layer_norm1.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.10.layer_norm1.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.10.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "vision_model.encoder.layers.10.mlp.fc1.bias : torch.Size([3072])\n",
      "vision_model.encoder.layers.10.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "vision_model.encoder.layers.10.mlp.fc2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.10.layer_norm2.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.10.layer_norm2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.11.self_attn.k_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.11.self_attn.k_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.11.self_attn.v_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.11.self_attn.v_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.11.self_attn.q_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.11.self_attn.q_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.11.self_attn.out_proj.weight : torch.Size([768, 768])\n",
      "vision_model.encoder.layers.11.self_attn.out_proj.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.11.layer_norm1.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.11.layer_norm1.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.11.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "vision_model.encoder.layers.11.mlp.fc1.bias : torch.Size([3072])\n",
      "vision_model.encoder.layers.11.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "vision_model.encoder.layers.11.mlp.fc2.bias : torch.Size([768])\n",
      "vision_model.encoder.layers.11.layer_norm2.weight : torch.Size([768])\n",
      "vision_model.encoder.layers.11.layer_norm2.bias : torch.Size([768])\n",
      "vision_model.post_layernorm.weight : torch.Size([768])\n",
      "vision_model.post_layernorm.bias : torch.Size([768])\n",
      "visual_projection.weight : torch.Size([512, 768])\n",
      "text_projection.weight : torch.Size([512, 512])\n"
     ]
    }
   ],
   "source": [
    "print_checkpoint(checkpoint_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d5897a6-99cf-412b-b2b5-6bb3d58dcd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model_to_checkpoint(checkpoint_state_dict):\n",
    "    \"\"\"Convert the openai clip vision model checkpoint keys\n",
    "        to vit (mae) vision model checkpoint keys\n",
    "    \"\"\"\n",
    "    converted_state_dict = {}\n",
    "    \n",
    "    # delete non vision model keys\n",
    "    for key in list(checkpoint_state_dict.keys()):\n",
    "        if not key.startswith(\"vision_model\"):\n",
    "            del checkpoint_state_dict[key]\n",
    "            continue\n",
    "\n",
    "    # embeddings\n",
    "    if \"vision_model.embeddings.class_embedding\" in checkpoint_state_dict:\n",
    "        converted_state_dict[\"cls_token\"] = checkpoint_state_dict.pop(\"vision_model.embeddings.class_embedding\")\n",
    "    \n",
    "    if \"vision_model.embeddings.position_embedding.weight\" in checkpoint_state_dict:\n",
    "        converted_state_dict[\"pos_embed\"] = checkpoint_state_dict.pop(\"vision_model.embeddings.position_embedding.weight\").unsqueeze(0)\n",
    "    \n",
    "    if \"vision_model.embeddings.patch_embedding.weight\" in checkpoint_state_dict:\n",
    "        converted_state_dict[\"patch_embed.proj.weight\"] = checkpoint_state_dict.pop(\"vision_model.embeddings.patch_embedding.weight\")\n",
    "    \n",
    "    #  post_layernorm -> norm\n",
    "    if \"vision_model.post_layernorm.weight\" in checkpoint_state_dict:\n",
    "        converted_state_dict[\"norm.weight\"] = checkpoint_state_dict.pop(\"vision_model.post_layernorm.weight\")\n",
    "    \n",
    "    if \"vision_model.post_layernorm.bias\" in checkpoint_state_dict:\n",
    "        converted_state_dict[\"norm.bias\"] = checkpoint_state_dict.pop(\"vision_model.post_layernorm.bias\")\n",
    "\n",
    "    # convert vsion model keys\n",
    "    for key, value in checkpoint_state_dict.items():\n",
    "        new_key = key\n",
    "        \n",
    "        # 处理 vision_model.encoder.layers -> blocks\n",
    "        if \"vision_model.encoder.layers\" in key:\n",
    "            new_key = new_key.replace(\"vision_model.encoder.layers\", \"blocks\")\n",
    "        \n",
    "        # 处理 self_attn -> attn\n",
    "        if \"self_attn\" in key:\n",
    "            new_key = new_key.replace(\"self_attn\", \"attn\")\n",
    "            \n",
    "            # 处理 q_proj, k_proj, v_proj 合并为 qkv\n",
    "            if \"q_proj.weight\" in key:\n",
    "                # 假设 q_proj, k_proj, v_proj 依次出现在 state_dict 中\n",
    "                q_weight = value\n",
    "                k_weight = checkpoint_state_dict[key.replace(\"q_proj\", \"k_proj\")]\n",
    "                v_weight = checkpoint_state_dict[key.replace(\"q_proj\", \"v_proj\")]\n",
    "                qkv_weight = torch.cat([q_weight, k_weight, v_weight], dim=0)\n",
    "                converted_state_dict[new_key.replace(\"q_proj.weight\", \"qkv.weight\")] = qkv_weight\n",
    "                continue\n",
    "            elif \"q_proj.bias\" in key:\n",
    "                # 假设 q_proj, k_proj, v_proj 依次出现在 state_dict 中\n",
    "                q_bias = value\n",
    "                k_bias = checkpoint_state_dict[key.replace(\"q_proj\", \"k_proj\")]\n",
    "                v_bias = checkpoint_state_dict[key.replace(\"q_proj\", \"v_proj\")]\n",
    "                qkv_bias = torch.cat([q_bias, k_bias, v_bias], dim=0)\n",
    "                converted_state_dict[new_key.replace(\"q_proj.bias\", \"qkv.bias\")] = qkv_bias\n",
    "                continue\n",
    "        \n",
    "        # 处理 out_proj -> proj\n",
    "        if \"out_proj\" in key:\n",
    "            new_key = new_key.replace(\"out_proj\", \"proj\")\n",
    "        \n",
    "        # 处理 layer_norm1 -> norm1, layer_norm2 -> norm2\n",
    "        new_key = new_key.replace(\"layer_norm1\", \"norm1\")\n",
    "        new_key = new_key.replace(\"layer_norm2\", \"norm2\")\n",
    "        \n",
    "        # 保存转换后的参数\n",
    "        converted_state_dict[new_key] = value\n",
    "    \n",
    "    return converted_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5dc0ca49-e703-4b78-a1c7-dfcdec067e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_checkpoint = convert_model_to_checkpoint(checkpoint_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "608e4094-e170-4e2f-b8f7-edd949152be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_embed : torch.Size([1, 197, 768])\n",
      "patch_embed.proj.weight : torch.Size([768, 3, 16, 16])\n",
      "norm.weight : torch.Size([768])\n",
      "norm.bias : torch.Size([768])\n",
      "vision_model.pre_layrnorm.weight : torch.Size([768])\n",
      "vision_model.pre_layrnorm.bias : torch.Size([768])\n",
      "blocks.0.attn.k_proj.weight : torch.Size([768, 768])\n",
      "blocks.0.attn.k_proj.bias : torch.Size([768])\n",
      "blocks.0.attn.v_proj.weight : torch.Size([768, 768])\n",
      "blocks.0.attn.v_proj.bias : torch.Size([768])\n",
      "blocks.0.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.0.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.0.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.0.attn.proj.bias : torch.Size([768])\n",
      "blocks.0.norm1.weight : torch.Size([768])\n",
      "blocks.0.norm1.bias : torch.Size([768])\n",
      "blocks.0.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.0.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.0.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.0.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.0.norm2.weight : torch.Size([768])\n",
      "blocks.0.norm2.bias : torch.Size([768])\n",
      "blocks.1.attn.k_proj.weight : torch.Size([768, 768])\n",
      "blocks.1.attn.k_proj.bias : torch.Size([768])\n",
      "blocks.1.attn.v_proj.weight : torch.Size([768, 768])\n",
      "blocks.1.attn.v_proj.bias : torch.Size([768])\n",
      "blocks.1.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.1.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.1.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.1.attn.proj.bias : torch.Size([768])\n",
      "blocks.1.norm1.weight : torch.Size([768])\n",
      "blocks.1.norm1.bias : torch.Size([768])\n",
      "blocks.1.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.1.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.1.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.1.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.1.norm2.weight : torch.Size([768])\n",
      "blocks.1.norm2.bias : torch.Size([768])\n",
      "blocks.2.attn.k_proj.weight : torch.Size([768, 768])\n",
      "blocks.2.attn.k_proj.bias : torch.Size([768])\n",
      "blocks.2.attn.v_proj.weight : torch.Size([768, 768])\n",
      "blocks.2.attn.v_proj.bias : torch.Size([768])\n",
      "blocks.2.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.2.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.2.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.2.attn.proj.bias : torch.Size([768])\n",
      "blocks.2.norm1.weight : torch.Size([768])\n",
      "blocks.2.norm1.bias : torch.Size([768])\n",
      "blocks.2.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.2.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.2.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.2.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.2.norm2.weight : torch.Size([768])\n",
      "blocks.2.norm2.bias : torch.Size([768])\n",
      "blocks.3.attn.k_proj.weight : torch.Size([768, 768])\n",
      "blocks.3.attn.k_proj.bias : torch.Size([768])\n",
      "blocks.3.attn.v_proj.weight : torch.Size([768, 768])\n",
      "blocks.3.attn.v_proj.bias : torch.Size([768])\n",
      "blocks.3.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.3.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.3.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.3.attn.proj.bias : torch.Size([768])\n",
      "blocks.3.norm1.weight : torch.Size([768])\n",
      "blocks.3.norm1.bias : torch.Size([768])\n",
      "blocks.3.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.3.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.3.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.3.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.3.norm2.weight : torch.Size([768])\n",
      "blocks.3.norm2.bias : torch.Size([768])\n",
      "blocks.4.attn.k_proj.weight : torch.Size([768, 768])\n",
      "blocks.4.attn.k_proj.bias : torch.Size([768])\n",
      "blocks.4.attn.v_proj.weight : torch.Size([768, 768])\n",
      "blocks.4.attn.v_proj.bias : torch.Size([768])\n",
      "blocks.4.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.4.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.4.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.4.attn.proj.bias : torch.Size([768])\n",
      "blocks.4.norm1.weight : torch.Size([768])\n",
      "blocks.4.norm1.bias : torch.Size([768])\n",
      "blocks.4.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.4.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.4.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.4.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.4.norm2.weight : torch.Size([768])\n",
      "blocks.4.norm2.bias : torch.Size([768])\n",
      "blocks.5.attn.k_proj.weight : torch.Size([768, 768])\n",
      "blocks.5.attn.k_proj.bias : torch.Size([768])\n",
      "blocks.5.attn.v_proj.weight : torch.Size([768, 768])\n",
      "blocks.5.attn.v_proj.bias : torch.Size([768])\n",
      "blocks.5.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.5.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.5.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.5.attn.proj.bias : torch.Size([768])\n",
      "blocks.5.norm1.weight : torch.Size([768])\n",
      "blocks.5.norm1.bias : torch.Size([768])\n",
      "blocks.5.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.5.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.5.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.5.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.5.norm2.weight : torch.Size([768])\n",
      "blocks.5.norm2.bias : torch.Size([768])\n",
      "blocks.6.attn.k_proj.weight : torch.Size([768, 768])\n",
      "blocks.6.attn.k_proj.bias : torch.Size([768])\n",
      "blocks.6.attn.v_proj.weight : torch.Size([768, 768])\n",
      "blocks.6.attn.v_proj.bias : torch.Size([768])\n",
      "blocks.6.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.6.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.6.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.6.attn.proj.bias : torch.Size([768])\n",
      "blocks.6.norm1.weight : torch.Size([768])\n",
      "blocks.6.norm1.bias : torch.Size([768])\n",
      "blocks.6.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.6.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.6.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.6.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.6.norm2.weight : torch.Size([768])\n",
      "blocks.6.norm2.bias : torch.Size([768])\n",
      "blocks.7.attn.k_proj.weight : torch.Size([768, 768])\n",
      "blocks.7.attn.k_proj.bias : torch.Size([768])\n",
      "blocks.7.attn.v_proj.weight : torch.Size([768, 768])\n",
      "blocks.7.attn.v_proj.bias : torch.Size([768])\n",
      "blocks.7.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.7.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.7.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.7.attn.proj.bias : torch.Size([768])\n",
      "blocks.7.norm1.weight : torch.Size([768])\n",
      "blocks.7.norm1.bias : torch.Size([768])\n",
      "blocks.7.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.7.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.7.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.7.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.7.norm2.weight : torch.Size([768])\n",
      "blocks.7.norm2.bias : torch.Size([768])\n",
      "blocks.8.attn.k_proj.weight : torch.Size([768, 768])\n",
      "blocks.8.attn.k_proj.bias : torch.Size([768])\n",
      "blocks.8.attn.v_proj.weight : torch.Size([768, 768])\n",
      "blocks.8.attn.v_proj.bias : torch.Size([768])\n",
      "blocks.8.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.8.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.8.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.8.attn.proj.bias : torch.Size([768])\n",
      "blocks.8.norm1.weight : torch.Size([768])\n",
      "blocks.8.norm1.bias : torch.Size([768])\n",
      "blocks.8.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.8.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.8.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.8.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.8.norm2.weight : torch.Size([768])\n",
      "blocks.8.norm2.bias : torch.Size([768])\n",
      "blocks.9.attn.k_proj.weight : torch.Size([768, 768])\n",
      "blocks.9.attn.k_proj.bias : torch.Size([768])\n",
      "blocks.9.attn.v_proj.weight : torch.Size([768, 768])\n",
      "blocks.9.attn.v_proj.bias : torch.Size([768])\n",
      "blocks.9.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.9.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.9.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.9.attn.proj.bias : torch.Size([768])\n",
      "blocks.9.norm1.weight : torch.Size([768])\n",
      "blocks.9.norm1.bias : torch.Size([768])\n",
      "blocks.9.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.9.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.9.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.9.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.9.norm2.weight : torch.Size([768])\n",
      "blocks.9.norm2.bias : torch.Size([768])\n",
      "blocks.10.attn.k_proj.weight : torch.Size([768, 768])\n",
      "blocks.10.attn.k_proj.bias : torch.Size([768])\n",
      "blocks.10.attn.v_proj.weight : torch.Size([768, 768])\n",
      "blocks.10.attn.v_proj.bias : torch.Size([768])\n",
      "blocks.10.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.10.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.10.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.10.attn.proj.bias : torch.Size([768])\n",
      "blocks.10.norm1.weight : torch.Size([768])\n",
      "blocks.10.norm1.bias : torch.Size([768])\n",
      "blocks.10.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.10.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.10.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.10.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.10.norm2.weight : torch.Size([768])\n",
      "blocks.10.norm2.bias : torch.Size([768])\n",
      "blocks.11.attn.k_proj.weight : torch.Size([768, 768])\n",
      "blocks.11.attn.k_proj.bias : torch.Size([768])\n",
      "blocks.11.attn.v_proj.weight : torch.Size([768, 768])\n",
      "blocks.11.attn.v_proj.bias : torch.Size([768])\n",
      "blocks.11.attn.qkv.weight : torch.Size([2304, 768])\n",
      "blocks.11.attn.qkv.bias : torch.Size([2304])\n",
      "blocks.11.attn.proj.weight : torch.Size([768, 768])\n",
      "blocks.11.attn.proj.bias : torch.Size([768])\n",
      "blocks.11.norm1.weight : torch.Size([768])\n",
      "blocks.11.norm1.bias : torch.Size([768])\n",
      "blocks.11.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "blocks.11.mlp.fc1.bias : torch.Size([3072])\n",
      "blocks.11.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "blocks.11.mlp.fc2.bias : torch.Size([768])\n",
      "blocks.11.norm2.weight : torch.Size([768])\n",
      "blocks.11.norm2.bias : torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "print_checkpoint(new_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0dfb6ebf-0a29-4dc6-8358-7b4729d13ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1141508/4100341605.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vit_checkpoint = torch.load('/jmain02/home/J2AD007/txk47/cxz00-txk47/cliport/checkpoints/Croco_vit_base16.pth')\n"
     ]
    }
   ],
   "source": [
    "vit_checkpoint = torch.load('/jmain02/home/J2AD007/txk47/cxz00-txk47/cliport/checkpoints/Croco_vit_base16.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3e654d7d-d707-48ca-bf4f-6072db3118c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_token : torch.Size([1, 1, 512])\n",
      "enc_pos_embed : torch.Size([196, 768])\n",
      "dec_pos_embed : torch.Size([196, 512])\n",
      "patch_embed.proj.weight : torch.Size([768, 3, 16, 16])\n",
      "patch_embed.proj.bias : torch.Size([768])\n",
      "enc_blocks.0.norm1.weight : torch.Size([768])\n",
      "enc_blocks.0.norm1.bias : torch.Size([768])\n",
      "enc_blocks.0.attn.qkv.weight : torch.Size([2304, 768])\n",
      "enc_blocks.0.attn.qkv.bias : torch.Size([2304])\n",
      "enc_blocks.0.attn.proj.weight : torch.Size([768, 768])\n",
      "enc_blocks.0.attn.proj.bias : torch.Size([768])\n",
      "enc_blocks.0.norm2.weight : torch.Size([768])\n",
      "enc_blocks.0.norm2.bias : torch.Size([768])\n",
      "enc_blocks.0.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "enc_blocks.0.mlp.fc1.bias : torch.Size([3072])\n",
      "enc_blocks.0.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "enc_blocks.0.mlp.fc2.bias : torch.Size([768])\n",
      "enc_blocks.1.norm1.weight : torch.Size([768])\n",
      "enc_blocks.1.norm1.bias : torch.Size([768])\n",
      "enc_blocks.1.attn.qkv.weight : torch.Size([2304, 768])\n",
      "enc_blocks.1.attn.qkv.bias : torch.Size([2304])\n",
      "enc_blocks.1.attn.proj.weight : torch.Size([768, 768])\n",
      "enc_blocks.1.attn.proj.bias : torch.Size([768])\n",
      "enc_blocks.1.norm2.weight : torch.Size([768])\n",
      "enc_blocks.1.norm2.bias : torch.Size([768])\n",
      "enc_blocks.1.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "enc_blocks.1.mlp.fc1.bias : torch.Size([3072])\n",
      "enc_blocks.1.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "enc_blocks.1.mlp.fc2.bias : torch.Size([768])\n",
      "enc_blocks.2.norm1.weight : torch.Size([768])\n",
      "enc_blocks.2.norm1.bias : torch.Size([768])\n",
      "enc_blocks.2.attn.qkv.weight : torch.Size([2304, 768])\n",
      "enc_blocks.2.attn.qkv.bias : torch.Size([2304])\n",
      "enc_blocks.2.attn.proj.weight : torch.Size([768, 768])\n",
      "enc_blocks.2.attn.proj.bias : torch.Size([768])\n",
      "enc_blocks.2.norm2.weight : torch.Size([768])\n",
      "enc_blocks.2.norm2.bias : torch.Size([768])\n",
      "enc_blocks.2.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "enc_blocks.2.mlp.fc1.bias : torch.Size([3072])\n",
      "enc_blocks.2.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "enc_blocks.2.mlp.fc2.bias : torch.Size([768])\n",
      "enc_blocks.3.norm1.weight : torch.Size([768])\n",
      "enc_blocks.3.norm1.bias : torch.Size([768])\n",
      "enc_blocks.3.attn.qkv.weight : torch.Size([2304, 768])\n",
      "enc_blocks.3.attn.qkv.bias : torch.Size([2304])\n",
      "enc_blocks.3.attn.proj.weight : torch.Size([768, 768])\n",
      "enc_blocks.3.attn.proj.bias : torch.Size([768])\n",
      "enc_blocks.3.norm2.weight : torch.Size([768])\n",
      "enc_blocks.3.norm2.bias : torch.Size([768])\n",
      "enc_blocks.3.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "enc_blocks.3.mlp.fc1.bias : torch.Size([3072])\n",
      "enc_blocks.3.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "enc_blocks.3.mlp.fc2.bias : torch.Size([768])\n",
      "enc_blocks.4.norm1.weight : torch.Size([768])\n",
      "enc_blocks.4.norm1.bias : torch.Size([768])\n",
      "enc_blocks.4.attn.qkv.weight : torch.Size([2304, 768])\n",
      "enc_blocks.4.attn.qkv.bias : torch.Size([2304])\n",
      "enc_blocks.4.attn.proj.weight : torch.Size([768, 768])\n",
      "enc_blocks.4.attn.proj.bias : torch.Size([768])\n",
      "enc_blocks.4.norm2.weight : torch.Size([768])\n",
      "enc_blocks.4.norm2.bias : torch.Size([768])\n",
      "enc_blocks.4.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "enc_blocks.4.mlp.fc1.bias : torch.Size([3072])\n",
      "enc_blocks.4.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "enc_blocks.4.mlp.fc2.bias : torch.Size([768])\n",
      "enc_blocks.5.norm1.weight : torch.Size([768])\n",
      "enc_blocks.5.norm1.bias : torch.Size([768])\n",
      "enc_blocks.5.attn.qkv.weight : torch.Size([2304, 768])\n",
      "enc_blocks.5.attn.qkv.bias : torch.Size([2304])\n",
      "enc_blocks.5.attn.proj.weight : torch.Size([768, 768])\n",
      "enc_blocks.5.attn.proj.bias : torch.Size([768])\n",
      "enc_blocks.5.norm2.weight : torch.Size([768])\n",
      "enc_blocks.5.norm2.bias : torch.Size([768])\n",
      "enc_blocks.5.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "enc_blocks.5.mlp.fc1.bias : torch.Size([3072])\n",
      "enc_blocks.5.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "enc_blocks.5.mlp.fc2.bias : torch.Size([768])\n",
      "enc_blocks.6.norm1.weight : torch.Size([768])\n",
      "enc_blocks.6.norm1.bias : torch.Size([768])\n",
      "enc_blocks.6.attn.qkv.weight : torch.Size([2304, 768])\n",
      "enc_blocks.6.attn.qkv.bias : torch.Size([2304])\n",
      "enc_blocks.6.attn.proj.weight : torch.Size([768, 768])\n",
      "enc_blocks.6.attn.proj.bias : torch.Size([768])\n",
      "enc_blocks.6.norm2.weight : torch.Size([768])\n",
      "enc_blocks.6.norm2.bias : torch.Size([768])\n",
      "enc_blocks.6.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "enc_blocks.6.mlp.fc1.bias : torch.Size([3072])\n",
      "enc_blocks.6.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "enc_blocks.6.mlp.fc2.bias : torch.Size([768])\n",
      "enc_blocks.7.norm1.weight : torch.Size([768])\n",
      "enc_blocks.7.norm1.bias : torch.Size([768])\n",
      "enc_blocks.7.attn.qkv.weight : torch.Size([2304, 768])\n",
      "enc_blocks.7.attn.qkv.bias : torch.Size([2304])\n",
      "enc_blocks.7.attn.proj.weight : torch.Size([768, 768])\n",
      "enc_blocks.7.attn.proj.bias : torch.Size([768])\n",
      "enc_blocks.7.norm2.weight : torch.Size([768])\n",
      "enc_blocks.7.norm2.bias : torch.Size([768])\n",
      "enc_blocks.7.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "enc_blocks.7.mlp.fc1.bias : torch.Size([3072])\n",
      "enc_blocks.7.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "enc_blocks.7.mlp.fc2.bias : torch.Size([768])\n",
      "enc_blocks.8.norm1.weight : torch.Size([768])\n",
      "enc_blocks.8.norm1.bias : torch.Size([768])\n",
      "enc_blocks.8.attn.qkv.weight : torch.Size([2304, 768])\n",
      "enc_blocks.8.attn.qkv.bias : torch.Size([2304])\n",
      "enc_blocks.8.attn.proj.weight : torch.Size([768, 768])\n",
      "enc_blocks.8.attn.proj.bias : torch.Size([768])\n",
      "enc_blocks.8.norm2.weight : torch.Size([768])\n",
      "enc_blocks.8.norm2.bias : torch.Size([768])\n",
      "enc_blocks.8.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "enc_blocks.8.mlp.fc1.bias : torch.Size([3072])\n",
      "enc_blocks.8.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "enc_blocks.8.mlp.fc2.bias : torch.Size([768])\n",
      "enc_blocks.9.norm1.weight : torch.Size([768])\n",
      "enc_blocks.9.norm1.bias : torch.Size([768])\n",
      "enc_blocks.9.attn.qkv.weight : torch.Size([2304, 768])\n",
      "enc_blocks.9.attn.qkv.bias : torch.Size([2304])\n",
      "enc_blocks.9.attn.proj.weight : torch.Size([768, 768])\n",
      "enc_blocks.9.attn.proj.bias : torch.Size([768])\n",
      "enc_blocks.9.norm2.weight : torch.Size([768])\n",
      "enc_blocks.9.norm2.bias : torch.Size([768])\n",
      "enc_blocks.9.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "enc_blocks.9.mlp.fc1.bias : torch.Size([3072])\n",
      "enc_blocks.9.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "enc_blocks.9.mlp.fc2.bias : torch.Size([768])\n",
      "enc_blocks.10.norm1.weight : torch.Size([768])\n",
      "enc_blocks.10.norm1.bias : torch.Size([768])\n",
      "enc_blocks.10.attn.qkv.weight : torch.Size([2304, 768])\n",
      "enc_blocks.10.attn.qkv.bias : torch.Size([2304])\n",
      "enc_blocks.10.attn.proj.weight : torch.Size([768, 768])\n",
      "enc_blocks.10.attn.proj.bias : torch.Size([768])\n",
      "enc_blocks.10.norm2.weight : torch.Size([768])\n",
      "enc_blocks.10.norm2.bias : torch.Size([768])\n",
      "enc_blocks.10.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "enc_blocks.10.mlp.fc1.bias : torch.Size([3072])\n",
      "enc_blocks.10.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "enc_blocks.10.mlp.fc2.bias : torch.Size([768])\n",
      "enc_blocks.11.norm1.weight : torch.Size([768])\n",
      "enc_blocks.11.norm1.bias : torch.Size([768])\n",
      "enc_blocks.11.attn.qkv.weight : torch.Size([2304, 768])\n",
      "enc_blocks.11.attn.qkv.bias : torch.Size([2304])\n",
      "enc_blocks.11.attn.proj.weight : torch.Size([768, 768])\n",
      "enc_blocks.11.attn.proj.bias : torch.Size([768])\n",
      "enc_blocks.11.norm2.weight : torch.Size([768])\n",
      "enc_blocks.11.norm2.bias : torch.Size([768])\n",
      "enc_blocks.11.mlp.fc1.weight : torch.Size([3072, 768])\n",
      "enc_blocks.11.mlp.fc1.bias : torch.Size([3072])\n",
      "enc_blocks.11.mlp.fc2.weight : torch.Size([768, 3072])\n",
      "enc_blocks.11.mlp.fc2.bias : torch.Size([768])\n",
      "enc_norm.weight : torch.Size([768])\n",
      "enc_norm.bias : torch.Size([768])\n",
      "decoder_embed.weight : torch.Size([512, 768])\n",
      "decoder_embed.bias : torch.Size([512])\n",
      "dec_blocks.0.norm1.weight : torch.Size([512])\n",
      "dec_blocks.0.norm1.bias : torch.Size([512])\n",
      "dec_blocks.0.attn.qkv.weight : torch.Size([1536, 512])\n",
      "dec_blocks.0.attn.qkv.bias : torch.Size([1536])\n",
      "dec_blocks.0.attn.proj.weight : torch.Size([512, 512])\n",
      "dec_blocks.0.attn.proj.bias : torch.Size([512])\n",
      "dec_blocks.0.cross_attn.projq.weight : torch.Size([512, 512])\n",
      "dec_blocks.0.cross_attn.projq.bias : torch.Size([512])\n",
      "dec_blocks.0.cross_attn.projk.weight : torch.Size([512, 512])\n",
      "dec_blocks.0.cross_attn.projk.bias : torch.Size([512])\n",
      "dec_blocks.0.cross_attn.projv.weight : torch.Size([512, 512])\n",
      "dec_blocks.0.cross_attn.projv.bias : torch.Size([512])\n",
      "dec_blocks.0.cross_attn.proj.weight : torch.Size([512, 512])\n",
      "dec_blocks.0.cross_attn.proj.bias : torch.Size([512])\n",
      "dec_blocks.0.norm2.weight : torch.Size([512])\n",
      "dec_blocks.0.norm2.bias : torch.Size([512])\n",
      "dec_blocks.0.norm3.weight : torch.Size([512])\n",
      "dec_blocks.0.norm3.bias : torch.Size([512])\n",
      "dec_blocks.0.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "dec_blocks.0.mlp.fc1.bias : torch.Size([2048])\n",
      "dec_blocks.0.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "dec_blocks.0.mlp.fc2.bias : torch.Size([512])\n",
      "dec_blocks.0.norm_y.weight : torch.Size([512])\n",
      "dec_blocks.0.norm_y.bias : torch.Size([512])\n",
      "dec_blocks.1.norm1.weight : torch.Size([512])\n",
      "dec_blocks.1.norm1.bias : torch.Size([512])\n",
      "dec_blocks.1.attn.qkv.weight : torch.Size([1536, 512])\n",
      "dec_blocks.1.attn.qkv.bias : torch.Size([1536])\n",
      "dec_blocks.1.attn.proj.weight : torch.Size([512, 512])\n",
      "dec_blocks.1.attn.proj.bias : torch.Size([512])\n",
      "dec_blocks.1.cross_attn.projq.weight : torch.Size([512, 512])\n",
      "dec_blocks.1.cross_attn.projq.bias : torch.Size([512])\n",
      "dec_blocks.1.cross_attn.projk.weight : torch.Size([512, 512])\n",
      "dec_blocks.1.cross_attn.projk.bias : torch.Size([512])\n",
      "dec_blocks.1.cross_attn.projv.weight : torch.Size([512, 512])\n",
      "dec_blocks.1.cross_attn.projv.bias : torch.Size([512])\n",
      "dec_blocks.1.cross_attn.proj.weight : torch.Size([512, 512])\n",
      "dec_blocks.1.cross_attn.proj.bias : torch.Size([512])\n",
      "dec_blocks.1.norm2.weight : torch.Size([512])\n",
      "dec_blocks.1.norm2.bias : torch.Size([512])\n",
      "dec_blocks.1.norm3.weight : torch.Size([512])\n",
      "dec_blocks.1.norm3.bias : torch.Size([512])\n",
      "dec_blocks.1.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "dec_blocks.1.mlp.fc1.bias : torch.Size([2048])\n",
      "dec_blocks.1.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "dec_blocks.1.mlp.fc2.bias : torch.Size([512])\n",
      "dec_blocks.1.norm_y.weight : torch.Size([512])\n",
      "dec_blocks.1.norm_y.bias : torch.Size([512])\n",
      "dec_blocks.2.norm1.weight : torch.Size([512])\n",
      "dec_blocks.2.norm1.bias : torch.Size([512])\n",
      "dec_blocks.2.attn.qkv.weight : torch.Size([1536, 512])\n",
      "dec_blocks.2.attn.qkv.bias : torch.Size([1536])\n",
      "dec_blocks.2.attn.proj.weight : torch.Size([512, 512])\n",
      "dec_blocks.2.attn.proj.bias : torch.Size([512])\n",
      "dec_blocks.2.cross_attn.projq.weight : torch.Size([512, 512])\n",
      "dec_blocks.2.cross_attn.projq.bias : torch.Size([512])\n",
      "dec_blocks.2.cross_attn.projk.weight : torch.Size([512, 512])\n",
      "dec_blocks.2.cross_attn.projk.bias : torch.Size([512])\n",
      "dec_blocks.2.cross_attn.projv.weight : torch.Size([512, 512])\n",
      "dec_blocks.2.cross_attn.projv.bias : torch.Size([512])\n",
      "dec_blocks.2.cross_attn.proj.weight : torch.Size([512, 512])\n",
      "dec_blocks.2.cross_attn.proj.bias : torch.Size([512])\n",
      "dec_blocks.2.norm2.weight : torch.Size([512])\n",
      "dec_blocks.2.norm2.bias : torch.Size([512])\n",
      "dec_blocks.2.norm3.weight : torch.Size([512])\n",
      "dec_blocks.2.norm3.bias : torch.Size([512])\n",
      "dec_blocks.2.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "dec_blocks.2.mlp.fc1.bias : torch.Size([2048])\n",
      "dec_blocks.2.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "dec_blocks.2.mlp.fc2.bias : torch.Size([512])\n",
      "dec_blocks.2.norm_y.weight : torch.Size([512])\n",
      "dec_blocks.2.norm_y.bias : torch.Size([512])\n",
      "dec_blocks.3.norm1.weight : torch.Size([512])\n",
      "dec_blocks.3.norm1.bias : torch.Size([512])\n",
      "dec_blocks.3.attn.qkv.weight : torch.Size([1536, 512])\n",
      "dec_blocks.3.attn.qkv.bias : torch.Size([1536])\n",
      "dec_blocks.3.attn.proj.weight : torch.Size([512, 512])\n",
      "dec_blocks.3.attn.proj.bias : torch.Size([512])\n",
      "dec_blocks.3.cross_attn.projq.weight : torch.Size([512, 512])\n",
      "dec_blocks.3.cross_attn.projq.bias : torch.Size([512])\n",
      "dec_blocks.3.cross_attn.projk.weight : torch.Size([512, 512])\n",
      "dec_blocks.3.cross_attn.projk.bias : torch.Size([512])\n",
      "dec_blocks.3.cross_attn.projv.weight : torch.Size([512, 512])\n",
      "dec_blocks.3.cross_attn.projv.bias : torch.Size([512])\n",
      "dec_blocks.3.cross_attn.proj.weight : torch.Size([512, 512])\n",
      "dec_blocks.3.cross_attn.proj.bias : torch.Size([512])\n",
      "dec_blocks.3.norm2.weight : torch.Size([512])\n",
      "dec_blocks.3.norm2.bias : torch.Size([512])\n",
      "dec_blocks.3.norm3.weight : torch.Size([512])\n",
      "dec_blocks.3.norm3.bias : torch.Size([512])\n",
      "dec_blocks.3.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "dec_blocks.3.mlp.fc1.bias : torch.Size([2048])\n",
      "dec_blocks.3.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "dec_blocks.3.mlp.fc2.bias : torch.Size([512])\n",
      "dec_blocks.3.norm_y.weight : torch.Size([512])\n",
      "dec_blocks.3.norm_y.bias : torch.Size([512])\n",
      "dec_blocks.4.norm1.weight : torch.Size([512])\n",
      "dec_blocks.4.norm1.bias : torch.Size([512])\n",
      "dec_blocks.4.attn.qkv.weight : torch.Size([1536, 512])\n",
      "dec_blocks.4.attn.qkv.bias : torch.Size([1536])\n",
      "dec_blocks.4.attn.proj.weight : torch.Size([512, 512])\n",
      "dec_blocks.4.attn.proj.bias : torch.Size([512])\n",
      "dec_blocks.4.cross_attn.projq.weight : torch.Size([512, 512])\n",
      "dec_blocks.4.cross_attn.projq.bias : torch.Size([512])\n",
      "dec_blocks.4.cross_attn.projk.weight : torch.Size([512, 512])\n",
      "dec_blocks.4.cross_attn.projk.bias : torch.Size([512])\n",
      "dec_blocks.4.cross_attn.projv.weight : torch.Size([512, 512])\n",
      "dec_blocks.4.cross_attn.projv.bias : torch.Size([512])\n",
      "dec_blocks.4.cross_attn.proj.weight : torch.Size([512, 512])\n",
      "dec_blocks.4.cross_attn.proj.bias : torch.Size([512])\n",
      "dec_blocks.4.norm2.weight : torch.Size([512])\n",
      "dec_blocks.4.norm2.bias : torch.Size([512])\n",
      "dec_blocks.4.norm3.weight : torch.Size([512])\n",
      "dec_blocks.4.norm3.bias : torch.Size([512])\n",
      "dec_blocks.4.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "dec_blocks.4.mlp.fc1.bias : torch.Size([2048])\n",
      "dec_blocks.4.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "dec_blocks.4.mlp.fc2.bias : torch.Size([512])\n",
      "dec_blocks.4.norm_y.weight : torch.Size([512])\n",
      "dec_blocks.4.norm_y.bias : torch.Size([512])\n",
      "dec_blocks.5.norm1.weight : torch.Size([512])\n",
      "dec_blocks.5.norm1.bias : torch.Size([512])\n",
      "dec_blocks.5.attn.qkv.weight : torch.Size([1536, 512])\n",
      "dec_blocks.5.attn.qkv.bias : torch.Size([1536])\n",
      "dec_blocks.5.attn.proj.weight : torch.Size([512, 512])\n",
      "dec_blocks.5.attn.proj.bias : torch.Size([512])\n",
      "dec_blocks.5.cross_attn.projq.weight : torch.Size([512, 512])\n",
      "dec_blocks.5.cross_attn.projq.bias : torch.Size([512])\n",
      "dec_blocks.5.cross_attn.projk.weight : torch.Size([512, 512])\n",
      "dec_blocks.5.cross_attn.projk.bias : torch.Size([512])\n",
      "dec_blocks.5.cross_attn.projv.weight : torch.Size([512, 512])\n",
      "dec_blocks.5.cross_attn.projv.bias : torch.Size([512])\n",
      "dec_blocks.5.cross_attn.proj.weight : torch.Size([512, 512])\n",
      "dec_blocks.5.cross_attn.proj.bias : torch.Size([512])\n",
      "dec_blocks.5.norm2.weight : torch.Size([512])\n",
      "dec_blocks.5.norm2.bias : torch.Size([512])\n",
      "dec_blocks.5.norm3.weight : torch.Size([512])\n",
      "dec_blocks.5.norm3.bias : torch.Size([512])\n",
      "dec_blocks.5.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "dec_blocks.5.mlp.fc1.bias : torch.Size([2048])\n",
      "dec_blocks.5.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "dec_blocks.5.mlp.fc2.bias : torch.Size([512])\n",
      "dec_blocks.5.norm_y.weight : torch.Size([512])\n",
      "dec_blocks.5.norm_y.bias : torch.Size([512])\n",
      "dec_blocks.6.norm1.weight : torch.Size([512])\n",
      "dec_blocks.6.norm1.bias : torch.Size([512])\n",
      "dec_blocks.6.attn.qkv.weight : torch.Size([1536, 512])\n",
      "dec_blocks.6.attn.qkv.bias : torch.Size([1536])\n",
      "dec_blocks.6.attn.proj.weight : torch.Size([512, 512])\n",
      "dec_blocks.6.attn.proj.bias : torch.Size([512])\n",
      "dec_blocks.6.cross_attn.projq.weight : torch.Size([512, 512])\n",
      "dec_blocks.6.cross_attn.projq.bias : torch.Size([512])\n",
      "dec_blocks.6.cross_attn.projk.weight : torch.Size([512, 512])\n",
      "dec_blocks.6.cross_attn.projk.bias : torch.Size([512])\n",
      "dec_blocks.6.cross_attn.projv.weight : torch.Size([512, 512])\n",
      "dec_blocks.6.cross_attn.projv.bias : torch.Size([512])\n",
      "dec_blocks.6.cross_attn.proj.weight : torch.Size([512, 512])\n",
      "dec_blocks.6.cross_attn.proj.bias : torch.Size([512])\n",
      "dec_blocks.6.norm2.weight : torch.Size([512])\n",
      "dec_blocks.6.norm2.bias : torch.Size([512])\n",
      "dec_blocks.6.norm3.weight : torch.Size([512])\n",
      "dec_blocks.6.norm3.bias : torch.Size([512])\n",
      "dec_blocks.6.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "dec_blocks.6.mlp.fc1.bias : torch.Size([2048])\n",
      "dec_blocks.6.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "dec_blocks.6.mlp.fc2.bias : torch.Size([512])\n",
      "dec_blocks.6.norm_y.weight : torch.Size([512])\n",
      "dec_blocks.6.norm_y.bias : torch.Size([512])\n",
      "dec_blocks.7.norm1.weight : torch.Size([512])\n",
      "dec_blocks.7.norm1.bias : torch.Size([512])\n",
      "dec_blocks.7.attn.qkv.weight : torch.Size([1536, 512])\n",
      "dec_blocks.7.attn.qkv.bias : torch.Size([1536])\n",
      "dec_blocks.7.attn.proj.weight : torch.Size([512, 512])\n",
      "dec_blocks.7.attn.proj.bias : torch.Size([512])\n",
      "dec_blocks.7.cross_attn.projq.weight : torch.Size([512, 512])\n",
      "dec_blocks.7.cross_attn.projq.bias : torch.Size([512])\n",
      "dec_blocks.7.cross_attn.projk.weight : torch.Size([512, 512])\n",
      "dec_blocks.7.cross_attn.projk.bias : torch.Size([512])\n",
      "dec_blocks.7.cross_attn.projv.weight : torch.Size([512, 512])\n",
      "dec_blocks.7.cross_attn.projv.bias : torch.Size([512])\n",
      "dec_blocks.7.cross_attn.proj.weight : torch.Size([512, 512])\n",
      "dec_blocks.7.cross_attn.proj.bias : torch.Size([512])\n",
      "dec_blocks.7.norm2.weight : torch.Size([512])\n",
      "dec_blocks.7.norm2.bias : torch.Size([512])\n",
      "dec_blocks.7.norm3.weight : torch.Size([512])\n",
      "dec_blocks.7.norm3.bias : torch.Size([512])\n",
      "dec_blocks.7.mlp.fc1.weight : torch.Size([2048, 512])\n",
      "dec_blocks.7.mlp.fc1.bias : torch.Size([2048])\n",
      "dec_blocks.7.mlp.fc2.weight : torch.Size([512, 2048])\n",
      "dec_blocks.7.mlp.fc2.bias : torch.Size([512])\n",
      "dec_blocks.7.norm_y.weight : torch.Size([512])\n",
      "dec_blocks.7.norm_y.bias : torch.Size([512])\n",
      "dec_norm.weight : torch.Size([512])\n",
      "dec_norm.bias : torch.Size([512])\n",
      "prediction_head.weight : torch.Size([768, 512])\n",
      "prediction_head.bias : torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "print_checkpoint(vit_checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3ff1cc79-2a32-4cbd-9c76-2e11b6442b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['cls_token', 'pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'blocks.0.attn.qkv.bias', 'blocks.1.attn.qkv.bias', 'blocks.2.attn.qkv.bias', 'blocks.3.attn.qkv.bias', 'blocks.4.attn.qkv.bias', 'blocks.5.attn.qkv.bias', 'blocks.6.attn.qkv.bias', 'blocks.7.attn.qkv.bias', 'blocks.8.attn.qkv.bias', 'blocks.9.attn.qkv.bias', 'blocks.10.attn.qkv.bias', 'blocks.11.attn.qkv.bias'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_checkpoint['model'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea7251-ff84-491a-a6fd-14a23cb30c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_checkpoint, '/jmain02/home/J2AD007/txk47/cxz00-txk47/cliport/checkpoints/clip_vit_base_patch16.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
